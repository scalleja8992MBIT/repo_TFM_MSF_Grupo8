{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook TFM: ETL - Transformación de datos CURATED a ENRICH\n",
    "<div style=\"background-color:#F2EDED;\">\n",
    "<br/>\n",
    "<div>\n",
    "<img src=\"https://uploads-ssl.webflow.com/614b1fe22fa8b90ef41aeffe/6265cb48f9496b1cefc9ab75_logotipo-mbit-39.png\" width=\"200px\" align=\"left\" CLASS=\"TextWrap\" style=\"background-color:#2a3f3f; margin-left: 10px;\">\n",
    "<img src=\"https://branding-guidelines.msf.es/esp/imgs/logo/Logo-01.jpg\" width=\"100px\" align=\"right\" CLASS=\"TextWrap\" style=\"background-color:#2a3f3f;\">\n",
    "</div>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<div>\n",
    "<h1><font color=\"#2a3f3f\" size=4 style=\"margin-left: 10px;\">MODELO DE PROBABILIDAD A TESTAR EN MÉDICOS SIN FRONTERAS</font></h1>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align: right; margin-right: 10px; margin-bottom: 10px;\">\n",
    "<font color=\"#2a3f3f\" size=3>Elio López Salamanca </font><br>\n",
    "<font color=\"#2a3f3f\" size=3>Sergio Israel Calleja Chimeno</font><br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=04c5a956-1a08-e670-80c5-be8c9e7f40a6) in (session=60c5a956-0962-c349-1495-123530f39e0e). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fae4adbb0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=3ac5a9a0-aa10-a37e-da6d-9b47cf84ec99) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfbb51007eb487a98d4c460e9e811b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=1ac5a9a4-1817-fd99-8284-692d665a0318) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078160717e33465bb84ed0941b35a92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "curated_paths = {\n",
    "    \"Contact\": \"s3://curated-msf-mbit/Contact/\",\n",
    "    \"Opportunity\": \"s3://curated-msf-mbit/Opportunity/\",\n",
    "    \"Campaign\": \"s3://curated-msf-mbit/Campaign/\",\n",
    "    \"RecurringDonation\": \"s3://curated-msf-mbit/RecurringDonation\"\n",
    "}\n",
    "    \n",
    "#enrichment_path = \"s3://enriched-msf-mbit/ContactsOpportunities/\"\n",
    "enrichment_path = \"s3://enriched-msf-mbit/ContactsOpportunitiesAgg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lee los datos de las tablas Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=f0c5a9a2-7bc6-cbf7-cb25-7e3cef4e6f1f) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e7d18467b44e609ac15f90968e1938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "def read_tables(table_paths: dict[str, str]) -> dict[str, DataFrame]:\n",
    "    try:\n",
    "        \n",
    "        tables = {}\n",
    "        for table_name, path in table_paths.items():\n",
    "            tables[table_name] = spark.read.format(\"delta\").load(path)\n",
    "        return tables\n",
    "        \n",
    "        if tables is None:\n",
    "            logger.error(\"Error: 'tables' is Null.\")\n",
    "        return\n",
    "\n",
    "        logger.info(\"Data extracted\")\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=dac5a9a3-2d3b-e112-7e54-40800246074c) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b367df4c0e4270bef953fb5f8ea16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "tables_ = read_tables(curated_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=56c5a9a3-8088-273e-b10a-12329a43ee2f) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd460c3780fd4d42b2592404e7edf4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "{'Contact': DataFrame[Id: string, Ltvscore: double, Program: string, Programaherencias: boolean, Programais: boolean, Legacyconfidentiality: boolean, Membertype: string, hash_key: string, processing_timestamp: timestamp], 'Opportunity': DataFrame[Id: string, Rating: string, Campaignid: string, PrimaryContact: string, RecurringDonation: string, Stagename: string, Program: string, Type: string, Typefundraisingcontribution: string, hash_key: string, processing_timestamp: timestamp], 'Campaign': DataFrame[Id: string, Campaigndonationreporting: string, Campaignentryreporting: string, Isemergency: boolean, Isonline: string, Objective: string, Objectivepublic: string, Segment: string, Status: string, hash_key: string, processing_timestamp: timestamp], 'RecurringDonation': DataFrame[Id: string, Isdeleted: boolean, Annualizedquota: double, Cancelationdate: date, Currentcampaign: string, Amount: double, Contact: string, InstallmentPeriod: string, PaidAmount: double, TotalPaidInstallments: double, hash_key: string, processing_timestamp: timestamp]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tables_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=f6c5a9a9-6762-75b6-b0bb-a2f0a804a62d) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b387a4623f463897412a84f4510a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "# Debido a que el anterior dataset contiene millones de registros,\n",
    "# procedemos a agregar los datos de la tabla Oportunidades para optimizar el dataset resultante\n",
    "\n",
    "def agg_oportunidades(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        group_by_columns = [\n",
    "            \"Rating\", \"Campaignid\", \"PrimaryContact\", \"RecurringDonation\",\n",
    "            \"Stagename\", \"Program\", \"Type\", \"Typefundraisingcontribution\"\n",
    "        ]\n",
    "        \n",
    "        aggregated_df = df.groupBy(group_by_columns).count().drop(\"count\")\n",
    "        \n",
    "        logger.info(\"DataFrame aggregated successfully.\")\n",
    "        return aggregated_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error aggregating DataFrame: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriquecer los datos de contactos con otras tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=eec5a9a3-b7fc-ef0b-706a-f4c65021dc4f) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1add756aa2ec4e4998a4562ecdabaa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "def create_enriched_dataset(opportunity: DataFrame, contact: DataFrame, \n",
    "                            campaign: DataFrame, recurring_donation: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Crea un dataset enriquecido a partir de varias tablas relacionadas.\n",
    "    \"\"\"\n",
    "    enriched_data = (\n",
    "        opportunity.alias(\"opp\")\n",
    "        .join(contact.alias(\"cnt\"), col(\"opp.PrimaryContact\") == col(\"cnt.Id\"), \"inner\")\n",
    "        .join(campaign.alias(\"cmp\"), col(\"opp.CampaignId\") == col(\"cmp.Id\"), \"inner\")\n",
    "        .join(recurring_donation.alias(\"rcd\"), \n",
    "              (col(\"opp.RecurringDonation\") == col(\"rcd.Id\")) & \n",
    "              (col(\"opp.PrimaryContact\") == col(\"rcd.Contact\")), \"left_outer\")\n",
    "    )\n",
    "    \n",
    "    opp_columns = [f\"opp.{col_name}\" for col_name in opportunity.columns]\n",
    "    cnt_columns = [f\"cnt.{col_name}\" for col_name in contact.columns]\n",
    "    cmp_columns = [f\"cmp.{col_name}\" for col_name in campaign.columns]\n",
    "    rcd_columns = [f\"rcd.{col_name}\" for col_name in recurring_donation.columns]\n",
    "\n",
    "    # Mapeo de alias a sus columnas correspondientes y prefijos deseados\n",
    "    alias_prefix_mapping = {\n",
    "        \"opp\": (opp_columns, \"Opp_\"),\n",
    "        \"cnt\": (cnt_columns, \"Cnt_\"),\n",
    "        \"cmp\": (cmp_columns, \"Cmp_\"),\n",
    "        \"rcd\": (rcd_columns, \"Rcd_\")\n",
    "    }\n",
    "\n",
    "    # Seleccionar y renombrar las columnas\n",
    "    selected_columns = [\n",
    "        col(alias_col).alias(f\"{prefix}{alias_col.split('.')[-1]}\")\n",
    "        for alias, (alias_cols, prefix) in alias_prefix_mapping.items()\n",
    "        for alias_col in alias_cols\n",
    "        if alias_col.split('.')[-1] not in [\"hash_key\", \"processing_timestamp\"]\n",
    "    ]\n",
    "\n",
    "    enriched_data = enriched_data.select(*selected_columns).dropDuplicates()\n",
    "    \n",
    "    print(\"Enriched columns generated: \")\n",
    "    print(enriched_data.printSchema())\n",
    "        \n",
    "    return enriched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga los datos enriquecidos en el destino en formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=9ac5a9a3-c141-7dc5-385c-24041ddc4748) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc4f68e478a4a30897fbf1f8d918ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "def write_dataset(df, output_path):\n",
    "    try:\n",
    "        \n",
    "        (\n",
    "            df.coalesce(5)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"Opp_Type\")\n",
    "            .save(output_path)\n",
    "        )\n",
    "    \n",
    "        logger.info(f\"Data written to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing data to {output_path}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ETL \"Enrichement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=1ec5a9a3-c97d-fb65-ef90-6c86b5cb7e49) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9b753eb60a4c4e8095837c987431f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "def run_enrichment(curated_paths: Dict[str, str], enrichment_path: str)-> None:\n",
    "    try:\n",
    "        # Extract\n",
    "        tables = read_tables(curated_paths)\n",
    "        \n",
    "        # Agragaciones\n",
    "        tables[\"OpportunityAgg\"] = agg_oportunidades(tables[\"Opportunity\"])\n",
    "        \n",
    "        # Transform / Enrich\n",
    "        enriched_data = create_enriched_dataset(\n",
    "             tables[\"OpportunityAgg\"], \n",
    "             tables[\"Contact\"], \n",
    "             tables[\"Campaign\"], \n",
    "             tables[\"RecurringDonation\"]\n",
    "         )\n",
    "\n",
    "        # Load\n",
    "        write_dataset(enriched_data, enrichment_path)\n",
    "    \n",
    "        logger.info(\"ETL 'enrichment' executed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ETL pipeline: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejectua ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=c4c5a9a5-0bfe-2e2c-ac27-e4825f402e32) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc438927a644362b5c30ea59fb5f697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "Enriched columns generated: \n",
      "root\n",
      " |-- Opp_Rating: string (nullable = true)\n",
      " |-- Opp_Campaignid: string (nullable = true)\n",
      " |-- Opp_PrimaryContact: string (nullable = true)\n",
      " |-- Opp_RecurringDonation: string (nullable = true)\n",
      " |-- Opp_Stagename: string (nullable = true)\n",
      " |-- Opp_Program: string (nullable = true)\n",
      " |-- Opp_Type: string (nullable = true)\n",
      " |-- Opp_Typefundraisingcontribution: string (nullable = true)\n",
      " |-- Cnt_Id: string (nullable = true)\n",
      " |-- Cnt_Ltvscore: double (nullable = true)\n",
      " |-- Cnt_Program: string (nullable = true)\n",
      " |-- Cnt_Programaherencias: boolean (nullable = true)\n",
      " |-- Cnt_Programais: boolean (nullable = true)\n",
      " |-- Cnt_Legacyconfidentiality: boolean (nullable = true)\n",
      " |-- Cnt_Membertype: string (nullable = true)\n",
      " |-- Cmp_Id: string (nullable = true)\n",
      " |-- Cmp_Campaigndonationreporting: string (nullable = true)\n",
      " |-- Cmp_Campaignentryreporting: string (nullable = true)\n",
      " |-- Cmp_Isemergency: boolean (nullable = true)\n",
      " |-- Cmp_Isonline: string (nullable = true)\n",
      " |-- Cmp_Objective: string (nullable = true)\n",
      " |-- Cmp_Objectivepublic: string (nullable = true)\n",
      " |-- Cmp_Segment: string (nullable = true)\n",
      " |-- Cmp_Status: string (nullable = true)\n",
      " |-- Rcd_Id: string (nullable = true)\n",
      " |-- Rcd_Isdeleted: boolean (nullable = true)\n",
      " |-- Rcd_Annualizedquota: double (nullable = true)\n",
      " |-- Rcd_Cancelationdate: date (nullable = true)\n",
      " |-- Rcd_Currentcampaign: string (nullable = true)\n",
      " |-- Rcd_Amount: double (nullable = true)\n",
      " |-- Rcd_Contact: string (nullable = true)\n",
      " |-- Rcd_InstallmentPeriod: string (nullable = true)\n",
      " |-- Rcd_PaidAmount: double (nullable = true)\n",
      " |-- Rcd_TotalPaidInstallments: double (nullable = true)\n",
      "\n",
      "None\n",
      "INFO:builtins:Data written to s3://enriched-msf-mbit/ContactsOpportunitiesAgg/\n",
      "INFO:builtins:ETL 'enrichment' executed successfully\n"
     ]
    }
   ],
   "source": [
    "run_enrichment(curated_paths, enrichment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=82c5a9a7-0d1f-96c5-bd30-314894f554a6) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbb2c344cb34e8f9feb53806a6539f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation 82c5a9a7-0d1f-96c5-bd30-314894f554a6 failed\n",
      "\n",
      "\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 159, in load\n",
      "    return self._df(self._jreader.load(path))\n",
      "  File \"/opt/amazon/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/amazon/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o249.load.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 4 times, most recent failure: Lost task 0.3 in stage 62.0 (TID 742) ([2600:1f18:aa1:3a0e:20d7:173b:91be:2a5e] executor 25): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:775)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:821)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:815)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: Could not read footer for file: FileStatus{path=s3://enriched-msf-mbit/ContactsOpportunities/03e5cc79-6ab1-4188-bf12-6e89abe93b0f.csv; isDirectory=false; length=110281644; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false; metadata={}}\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:726)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:788)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\n",
      "\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n",
      "\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n",
      "\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n",
      "\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n",
      "Caused by: java.lang.RuntimeException: s3://enriched-msf-mbit/ContactsOpportunities/03e5cc79-6ab1-4188-bf12-6e89abe93b0f.csv is not a Parquet file. Expected magic number at tail, but found [111, 110, 34, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:564)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:782)\n",
      "\t... 13 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2559)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2558)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2558)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1200)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1200)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1200)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2740)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2729)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:978)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:825)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:173)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:775)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:821)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:815)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.io.IOException: Could not read footer for file: FileStatus{path=s3://enriched-msf-mbit/ContactsOpportunities/03e5cc79-6ab1-4188-bf12-6e89abe93b0f.csv; isDirectory=false; length=110281644; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false; metadata={}}\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:726)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:788)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\n",
      "\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n",
      "\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n",
      "\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n",
      "\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n",
      "Caused by: java.lang.RuntimeException: s3://enriched-msf-mbit/ContactsOpportunities/03e5cc79-6ab1-4188-bf12-6e89abe93b0f.csv is not a Parquet file. Expected magic number at tail, but found [111, 110, 34, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:564)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:782)\n",
      "\t... 13 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_read_parquet = spark.read.format(\"parquet\").load(\"s3://enriched-msf-mbit/ContactsOpportunities/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=14c5a9a6-0e38-67c9-1c40-45367979ae7a) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f469515b6c28437bb7173ce0dddb7ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "2682548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Despúes de la agregación\n",
    "df_read = spark.read.format(\"delta\").load(\"s3://enriched-msf-mbit/ContactsOpportunitiesAgg/\")\n",
    "df_read.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=9cc5a9ab-d657-c13d-39e8-ccf9a34fedff) in (session=c2c5a996-5161-6f58-6dde-c9cff6427c43). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee9480be1374f4cbe78ffd22ef26e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "df_single_partition = df_read.repartition(1)\n",
    "\n",
    "df_single_partition.write.csv(\"s3://mbit-oct22-msf-data-ealopezs/Dataset-result/\", mode=\"overwrite\", sep=\";\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
